{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4532a6a-899c-42ae-a5bc-e516b01ef9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install spacy_syllables\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc279d1b-6eec-4cad-ad9f-6ce1321fc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "#Importing everything from NLP Week 1 - following that as a guide for now\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.width = 150\n",
    "RANDOM_SEED = 696\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy_syllables import SpacySyllables\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795f176-52ee-45a6-aee2-4ce5841a6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_option(\"display.width\")\n",
    "\n",
    "# using https://spacy.io/universe/project/spacy_syllables\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba539b-3b9d-4ba8-936d-4032df219ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiLarge_Train_df = pd.read_csv(r'assets/WikiLarge_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad191e-c8d0-44bc-8dbd-e62cea124a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_pat = re.compile(r'\\b\\s+\\b')\n",
    "split_pat = re.compile(r'\\w+')\n",
    "\n",
    "WikiLarge_Train_df['og_split'] = WikiLarge_Train_df['original_text'].parallel_apply(lambda x: re.findall(split_pat, x))\n",
    "WikiLarge_Train_df['total_words'] = WikiLarge_Train_df['og_split'].parallel_apply(lambda x: len(x))\n",
    "WikiLarge_Train_df['long_words'] = WikiLarge_Train_df['og_split'].parallel_apply(lambda x: len([y for y in x if len(y) > 7]))\n",
    "WikiLarge_Train_df['total_sentences'] = 1\n",
    "WikiLarge_Train_df['total_characters'] = WikiLarge_Train_df['og_split'].parallel_apply(lambda x: sum([len(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef4775-03d3-46e1-8dd7-0f6026a1e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs slow\n",
    "# Get Syllables\n",
    "WikiLarge_Train_df['syl_list'] = WikiLarge_Train_df['original_text'].parallel_apply(lambda x: [token._.syllables_count for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80524290-0a97-4f36-a1f1-071991055649",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiLarge_Train_df['syl_list'] = WikiLarge_Train_df['syl_list'].parallel_apply(lambda x: [y for y in x if y is not None])\n",
    "WikiLarge_Train_df['total_syllables'] = WikiLarge_Train_df['syl_list'].parallel_apply(lambda x: sum(x))\n",
    "WikiLarge_Train_df['total_polysyllables'] = WikiLarge_Train_df['syl_list'].parallel_apply(lambda x: sum([1 for y in x if y>2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4313dc7-3994-4896-985a-156d21c4dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiLarge_Train_df['total_unique_words'] = WikiLarge_Train_df['og_split'].parallel_apply(lambda x: len(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7b532-08cd-4583-a970-82c76829028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esitmates the years of formal education a person needs to understand the tet on first reading.\n",
    "WikiLarge_Train_df['gfi'] = 0.4 * (WikiLarge_Train_df['total_words'] + 100 * WikiLarge_Train_df['long_words'])\n",
    "# WikiLarge_Train_df['gfi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce913b-2136-4949-8515-6cdb9f8be689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRE ( Flesch reading ease) assigns higher values to more readable texts.\n",
    "WikiLarge_Train_df['fre'] = 206.835 - 1.015*(WikiLarge_Train_df['total_words']) - 84.6 * (WikiLarge_Train_df['total_syllables']/WikiLarge_Train_df['total_words'])\n",
    "# WikiLarge_Train_df['fre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b65a3-323d-4927-84d5-73e44e9f55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FKGL) Flesch-Kincaid grade level is the number of years of education generally required to understand the text for which the formula was calculated\n",
    "WikiLarge_Train_df['fkgl'] = 0.39 * (WikiLarge_Train_df['total_words']) + 11.8 * (WikiLarge_Train_df['total_syllables']/WikiLarge_Train_df['total_words']) - 15.59\n",
    "# WikiLarge_Train_df['fkgl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414050f-3341-4bfd-8234-f4795d720059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI (Automated readability index) years of education required to understand the text\n",
    "WikiLarge_Train_df['ari'] = 4.71 * (WikiLarge_Train_df['total_characters'] / WikiLarge_Train_df['total_words']) + 0.5 * (WikiLarge_Train_df['total_words']) - 21.43\n",
    "# WikiLarge_Train_df['ari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d717911-2dd0-4893-ae25-ec634e8aeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOG (simple measurement of Gobbledygook) roughly corresponds to the number of years of education needed to understand the text\n",
    "WikiLarge_Train_df['smog'] = 1.0430 * np.sqrt(WikiLarge_Train_df['total_polysyllables'] * 30) + 3.1291\n",
    "# WikiLarge_Train_df['smog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58dedf-6245-4f6c-a7c7-a9fda33e4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTR (Type Token Ratio) (number of unique words / number of words)\n",
    "WikiLarge_Train_df['ttr'] = WikiLarge_Train_df['total_unique_words'] / WikiLarge_Train_df['total_words']\n",
    "\n",
    "# RTTR (root type token ratio)\n",
    "WikiLarge_Train_df['rttr'] = WikiLarge_Train_df['total_unique_words'] / np.sqrt(WikiLarge_Train_df['total_words'])\n",
    "\n",
    "# CTTR (corrected type token ratio)\n",
    "WikiLarge_Train_df['cttr'] = WikiLarge_Train_df['total_unique_words'] / np.sqrt(2 * WikiLarge_Train_df['total_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213e5b5-432f-427f-9bb2-968a5901fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSTTR is the average TTR for each non-overlapping segment of equal size\n",
    "# Assuming a 300 wpm average reading rate, we can assume a 5 wps = 300 wpm / 60 s reading rate. \n",
    "# Therefore, using a segment size of 5 is an appropriate window.\n",
    "# http://crr.ugent.be/papers/Brysbaert_JML_2019_Reading_rate.pdf\n",
    "\n",
    "def msttr_helper(lst:list, segment_size:int=np.NaN):\n",
    "    if np.isnan(segment_size):\n",
    "        segment_size=len(lst) \n",
    "    lst = [x.lower() for x in lst]\n",
    "    segments = [lst[i*segment_size: i*segment_size + segment_size] for i in range(int(np.ceil(len(lst)/segment_size)))]\n",
    "    segment_ttr_vals = [len(set(x)) / segment_size if len(x) == segment_size else len(set(x)) / len(x) for x in segments]\n",
    "    \n",
    "    return np.sum(segment_ttr_vals)/len(segment_ttr_vals)\n",
    "\n",
    "WikiLarge_Train_df['5gram_msttr'] = WikiLarge_Train_df['og_split'].parallel_apply(msttr_helper, segment_size=5)\n",
    "WikiLarge_Train_df['3gram_msttr'] = WikiLarge_Train_df['og_split'].parallel_apply(msttr_helper, segment_size=3)\n",
    "WikiLarge_Train_df['2gram_msttr'] = WikiLarge_Train_df['og_split'].parallel_apply(msttr_helper, segment_size=2)\n",
    "\n",
    "# WikiLarge_Train_df[['5gram_msttr', '3gram_msttr', '2gram_msttr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42188e-4cb4-4741-8f38-a72d2ea95ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATTR is the average TTR for all possible overlapping segments of equal size. \n",
    "# Assuming a 300 wpm average reading rate, we can assume a 5 wps = 300 wpm / 60 s reading rate. \n",
    "# Therefore, using a segment size of 5 is an appropriate window.\n",
    "# http://crr.ugent.be/papers/Brysbaert_JML_2019_Reading_rate.pdf\n",
    "\n",
    "def mattr_helper(lst:list, segment_size:int=np.NaN):\n",
    "    if np.isnan(segment_size):\n",
    "        segment_size=len(lst) \n",
    "    lst = [x.lower() for x in lst]\n",
    "    segments = [lst[i: i + segment_size] for i in range(len(lst))]\n",
    "    segment_ttr_vals = [len(set(x)) / segment_size if len(x) == segment_size else len(set(x))/len(x) for x in segments ]\n",
    "    \n",
    "    return np.sum(segment_ttr_vals)/len(segment_ttr_vals)\n",
    "\n",
    "WikiLarge_Train_df['5gram_mattr'] = WikiLarge_Train_df['og_split'].parallel_apply(mattr_helper, segment_size=5)\n",
    "WikiLarge_Train_df['len_ngram_mattr'] = WikiLarge_Train_df['og_split'].parallel_apply(mattr_helper)\n",
    "\n",
    "\n",
    "# WikiLarge_Train_df[['5gram_mattr', 'len_ngram_mattr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee8fb7-2fed-4e7c-adb3-167f0b365fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper calculates the moving average of syllables.\n",
    "# Assuming a 300 wpm average reading rate, we can assume a 5 wps = 300 wpm / 60 s reading rate. \n",
    "# Therefore, using a segment size of 5 is an appropriate window.\n",
    "# http://crr.ugent.be/papers/Brysbaert_JML_2019_Reading_rate.pdf\n",
    "\n",
    "def ma_syl_helper(lst:list, segment_size:int=np.NaN):\n",
    "    if np.isnan(segment_size):\n",
    "        segment_size=len(lst) \n",
    "    segments = [lst[i: i + segment_size] for i in range(len(lst))]\n",
    "    segment_ttr_vals = [sum(x) / segment_size if len(x) == segment_size else sum(x)/len(x) for x in segments ]\n",
    "    \n",
    "    return np.sum(segment_ttr_vals)/len(segment_ttr_vals)\n",
    "\n",
    "WikiLarge_Train_df['5gram_ma_syl'] = WikiLarge_Train_df['syl_list'].parallel_apply(ma_syl_helper, segment_size=5)\n",
    "WikiLarge_Train_df['len_ngram_ma_syl'] = WikiLarge_Train_df['syl_list'].parallel_apply(ma_syl_helper)\n",
    "WikiLarge_Train_df['syl_mean'] = WikiLarge_Train_df['syl_list'].parallel_apply(np.mean)\n",
    "WikiLarge_Train_df['syl_std'] = WikiLarge_Train_df['syl_list'].parallel_apply(np.std)\n",
    "\n",
    "\n",
    "# WikiLarge_Train_df[['5gram_ma_syl', 'len_ngram_ma_syl', 'syl_mean', 'syl_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042d09f-2436-42a9-a8ca-099a8c7c7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiLarge_Train_df.iloc[:, 3:].to_csv('score_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
