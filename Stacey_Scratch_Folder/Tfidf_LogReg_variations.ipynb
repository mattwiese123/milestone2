{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "#Importing everything from NLP Week 1 - following that as a guide for now\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.width = 150\n",
    "RANDOM_SEED = 696\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiLarge_Train_df = pd.read_csv(r'/Users/staceybruestle/Documents/Documents/Education/MADS/Coursework/aSIADS696- Milestone 2/Readability Project/Data/WikiLarge_Train.csv')#, \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df, test_df = np.split(WikiLarge_Train_df.sample(frac=1, random_state= RANDOM_SEED), \n",
    "                       [int(.8*len(WikiLarge_Train_df)), int(.9*len(WikiLarge_Train_df))], axis = 0)\n",
    "#Make list of labels\n",
    "y_train = list(train_df.label)\n",
    "y_dev = list(dev_df.label)\n",
    "y_test = list(test_df.label)\n",
    "# Shapes: 333414, 41677, 41677 all with 5 columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dummy Classifiers to use as reference for other scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyClassifierScores(train_df, dev_df, vectorizer, min_df=1, stop_words= None, strategy='uniform'):\n",
    "    # Stratgeies 'uniform' and 'most_frequent'\n",
    "    X_train = vectorizer.fit_transform(train_df.original_text)\n",
    "    X_dev = vectorizer.transform(dev_df.original_text)\n",
    "\n",
    "    dummy = DummyClassifier(strategy= strategy, random_state = RANDOM_SEED, constant=None)\n",
    "    dummy.fit(X_train, y_train)\n",
    "\n",
    "    # Generate the predictions\n",
    "    dev_preds = dummy.predict(X_dev)\n",
    "\n",
    "    # Score the predictions\n",
    "    f1_dummy = f1_score(y_dev, dev_preds, average='macro')\n",
    "\n",
    "    return f1_dummy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for the steps so we can run it for various amounts of data to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function returns the macro-averaged F1 score on the dev data and the dummy if requested\n",
    "def train_and_score(train_df, dev_df, min_df=1, max_df = 1.0, max_iter=100, C=1.0, \\\n",
    "                    ngram_range=(1,1), stop_words= None, dummy='no', strategy='uniform'):\n",
    "    # Fit a new TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(min_df= min_df, max_df= max_df, stop_words= None, ngram_range= ngram_range)\n",
    "    X_train = vectorizer.fit_transform(train_df.original_text)\n",
    "\n",
    "    #Get the labels\n",
    "    y_train = list(train_df.label)\n",
    "\n",
    "    #Fit the data to a Logistic Regression Classifier\n",
    "    clf = LogisticRegression(random_state=RANDOM_SEED, max_iter = max_iter, C= C, multi_class='ovr', solver= 'newton-cholesky')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Generate the dev data\n",
    "    X_dev = vectorizer.transform(dev_df.original_text)\n",
    "    y_dev = list(dev_df.label)\n",
    "\n",
    "    # Generate the predictions\n",
    "    lr_tiny_dev_preds = clf.predict(X_dev)\n",
    "\n",
    "    # Score the predictions\n",
    "    f1 = f1_score(y_dev, lr_tiny_dev_preds, average='macro')\n",
    "\n",
    "    if dummy== 'yes':\n",
    "        f1 = (f1, dummyClassifierScores(train_df, dev_df, vectorizer, min_df, stop_words, strategy= strategy))\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All defaults with uniform dummy:\")\n",
    "train_and_score(train_df, dev_df, dummy='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create the vectorizer to run dummyClassifierScores by itself\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(\"All defaults - most frequent dummy:\", \\\n",
    "dummyClassifierScores(train_df, dev_df, vectorizer, strategy= 'most_frequent') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"default with stopwords:\", \\\n",
    "train_and_score(train_df, dev_df, stop_words= 'english') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_score(train_df, dev_df, min_df=1, max_df = 1.0, max_iter=100, C=1.0, \\\n",
    "#                     ngram_range=(1,1), stop_words= None, dummy='no', strategy='uniform')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e6f7cd0ebba2aa2c388bbed66ebc49096d476d4216b999bf7270ff51b8a6e94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
